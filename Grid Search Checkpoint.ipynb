{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bb86d4b",
   "metadata": {},
   "source": [
    "Todo\n",
    "* delete the iterated hyperparameters save it in pickel or json\n",
    "* add concurrency- after first .fit start the next one + do the suff for the previous one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8284a5ae",
   "metadata": {},
   "source": [
    "## Sample Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a2c2f37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20c65ed",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dcde2137",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('diabetes.csv')\n",
    "X=data.drop(\"Outcome\",axis=1)\n",
    "y=data['Outcome']\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=42)\n",
    "\n",
    "svc_para = {'kernel': ['sigmoid',\"linear\"], 'gamma': [1, 10]}\n",
    "para = ParameterGrid(svc_para)\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "model = SVC()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244786c9",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f276234a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data =pd.read_csv('USA_Housing.csv')\n",
    "# X=data.drop([\"Address\",'Price'], axis=1)\n",
    "# y=data['Price']\n",
    "# X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=42)\n",
    "\n",
    "# knn_reg_para = {\"n_neighbors\" : [1,2,3,4,5], \"weights\" : [\"uniform\", \"distance\"]}\n",
    "# para = ParameterGrid(knn_reg_para)\n",
    "\n",
    "# from sklearn.neighbors import KNeighborsRegressor\n",
    "# model = KNeighborsRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "98cab456",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.base import is_classifier, is_regressor\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "import sys,csv,os\n",
    "\n",
    "class GridCheckpoint:\n",
    "\n",
    "    def __init__(self,model, params, X_train, X_test, y_train, y_test):\n",
    "        self.model = model\n",
    "        self.params = params\n",
    "        self.X_train = X_train\n",
    "        self.X_test = X_test\n",
    "        self.y_train = y_train\n",
    "        self.y_test = y_test\n",
    "        self.save_file = 'history.csv'\n",
    "        self.save_data = {'Hyperparameters' : None, 'Train Accuracy' : None, \"Test Accuracy\" : None, \"Train Loss\" : None, \"Test Loss\" : None}\n",
    "        \n",
    "    def save_log(self,data_arr):\n",
    "        if not os.path.exists(\"history.csv\") or os.stat(\"history.csv\").st_size == 0: # create save file\n",
    "            with open(self.save_file, 'w',newline='') as file: # newline to avoid space\n",
    "                writer = csv.DictWriter(file, fieldnames=list(data_arr.keys()))\n",
    "                writer.writeheader()\n",
    "                file.close()\n",
    "\n",
    "        with open(self.save_file, 'a') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(data_arr.values())\n",
    "            file.close()\n",
    "                 \n",
    "    def run(self):\n",
    "        for p in self.params:\n",
    "            self.save_data[list(self.save_data)[0]] = str(p)\n",
    "            print(\"{} - {}\\n\".format(list(self.save_data)[0], self.save_data[list(self.save_data)[0]]))\n",
    "            \n",
    "            if is_classifier(self.model):\n",
    "                if not(self.model.get_params()[\"probability\"]):  # probability=false in SVC causes error in predict_proba in self.score if is_classifier\n",
    "                    model = self.model.__class__(**p,probability=True).fit(self.X_train,self.y_train) # must use **kwargs when using ParameterGrid\n",
    "            else:\n",
    "                model = self.model.__class__(**p).fit(self.X_train,self.y_train)\n",
    "                \n",
    "            self.score(model)\n",
    "            print(\"\\t------------------------------------\")\n",
    "            \n",
    "    def score(self,fitted_model):\n",
    "        score_vals = []\n",
    "\n",
    "        score_vals.append(fitted_model.score(self.X_train,self.y_train))\n",
    "        score_vals.append(fitted_model.score(self.X_test,self.y_test))\n",
    "        \n",
    "        if is_classifier(self.model): # For classifier\n",
    "            score_vals.append(log_loss(self.y_train, fitted_model.predict_proba(X_train)))\n",
    "            score_vals.append(log_loss(self.y_test, fitted_model.predict_proba(X_test)))\n",
    "        \n",
    "        if is_regressor(self.model): # For Regressor\n",
    "            score_vals.append(mean_squared_error(self.y_train, fitted_model.predict(X_train)))\n",
    "            score_vals.append(mean_squared_error(self.y_test, fitted_model.predict(X_test)))  \n",
    "\n",
    "        for i in range(len(self.save_data)):\n",
    "            if i == 0:\n",
    "                # print(\"{} - {}\".format(list(self.save_data)[i], self.save_data[list(self.save_data)[i]])) # show before to see which hyper takes times\n",
    "                pass\n",
    "            else:\n",
    "                self.save_data[list(self.save_data)[i]] = score_vals[i-1]\n",
    "                print(\"{} | {}\".format(list(self.save_data)[i], self.save_data[list(self.save_data)[i]]))\n",
    "\n",
    "        self.save_log(self.save_data)\n",
    "\n",
    "    def best_params(self, tolerate_overfit = 0.10): # percentage\n",
    "        df = pd.read_csv(self.save_file)\n",
    "        score_mean = []\n",
    "        score_std = []\n",
    "        found_best = False\n",
    "\n",
    "        for i in range(len(df)):\n",
    "            score_mean.append(df[[list(self.save_data)[1],list(self.save_data)[2]]].iloc[i].mean())\n",
    "            score_std.append(df[[list(self.save_data)[1],list(self.save_data)[2]]].iloc[i].std())\n",
    "\n",
    "        for i in range(len(score_mean)):\n",
    "            index = score_mean.index(sorted(score_mean, reverse=True)[i])\n",
    "            print(index)\n",
    "            if score_std[index] <= (tolerate_overfit/2): # convert tolerate_overfit to percent, n/2% on each side of mean\n",
    "                print(df.iloc[index])\n",
    "                found_best = True\n",
    "                break\n",
    "    \n",
    "        if not found_best:\n",
    "            print(\"All model overfits over {}%\".format(tolerate_overfit*100))\n",
    "\n",
    "    def display_logs(self):\n",
    "        if not os.path.exists(\"history.csv\") or os.stat(\"history.csv\").st_size == 0:\n",
    "            print(\"No logs found\")\n",
    "        else:\n",
    "            print(pd.read_csv(self.save_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "374d7f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridCheckpoint(model, para,X_train,X_test,y_train,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fc00e52a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters - {'gamma': 1, 'kernel': 'sigmoid'}\n",
      "\n",
      "Train Accuracy | 0.6499068901303539\n",
      "Test Accuracy | 0.6536796536796536\n",
      "Train Loss | 0.6475158662194473\n",
      "Test Loss | 0.6452200115486163\n",
      "\t------------------------------------\n",
      "Hyperparameters - {'gamma': 1, 'kernel': 'linear'}\n",
      "\n",
      "Train Accuracy | 0.7802607076350093\n",
      "Test Accuracy | 0.7445887445887446\n",
      "Train Loss | 0.46424656438735207\n",
      "Test Loss | 0.5082577588989724\n",
      "\t------------------------------------\n",
      "Hyperparameters - {'gamma': 10, 'kernel': 'sigmoid'}\n",
      "\n",
      "Train Accuracy | 0.6499068901303539\n",
      "Test Accuracy | 0.6536796536796536\n",
      "Train Loss | 0.6475158662194473\n",
      "Test Loss | 0.6452200115486163\n",
      "\t------------------------------------\n",
      "Hyperparameters - {'gamma': 10, 'kernel': 'linear'}\n",
      "\n",
      "Train Accuracy | 0.7802607076350093\n",
      "Test Accuracy | 0.7445887445887446\n",
      "Train Loss | 0.46141876728790804\n",
      "Test Loss | 0.5114425278424826\n",
      "\t------------------------------------\n"
     ]
    }
   ],
   "source": [
    "grid.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "abd61353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Hyperparameters    {'gamma': 1, 'kernel': 'linear'}\n",
      "Train Accuracy                             0.780261\n",
      "Test Accuracy                              0.744589\n",
      "Train Loss                                 0.464247\n",
      "Test Loss                                  0.508258\n",
      "Name: 1, dtype: object\n"
     ]
    }
   ],
   "source": [
    "grid.best_params()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
